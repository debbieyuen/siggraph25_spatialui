import { Steps } from 'nextra/components'
import { Callout } from 'nextra/components'

# Object Detection

## Getting Started 
<Steps>

### Step 1: CombinedRealityView.swift
Navigate to `squiggly/Views/Main/CombinedRealityView.swift`.

### Step 2: Object Tab in Toolbar
On the Apple Vision Pros, you will find the toolbar on the left. In this window, we have 3 toolbars including the left toolbar, bottom toolbar, and the top ornament. Select the `Photos` Tab to open up the tab. 

### Step 3: Crayon Box and Blue Crayon
Once you are in the tab, you may try choosing a photo of your choice to classify. By default we are using Apple's built-in Vision Framework with `VNClassifyImageRequest()`. 

### Step 4: Draw Digits, Stars, or Hearts
Once you are in the tab, you may try choosing a photo of your choice to classify. By default we are using Apple's built-in Vision Framework with `VNClassifyImageRequest()`. 

![Apple Vision Pro](../../components/public/images/imageclassify/classify3.png)

</Steps>

## Overview

An image classifier is a machine learning model that recognizes images. When you give it an image, it responds with a category label for that image. You train an image classifier by showing it many examples of images youâ€™ve already labeled. For example, you can train an image classifier to recognize animals by gathering photos of elephants, giraffes, lions, and so on. After the image classifier finishes training, you assess its accuracy and, if it performs well enough, save it as a Core ML model file. You then import the model file into your Xcode project to use the image classifier in your app. CreateML is a tool that we will be using to support us through this process.

## Reference Objects
First lets start off with picking a 3D model. In our case, we got 24 count crayon boxes from Target. Luckily, they are on sale for 0.50 cents (USD)!!

![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection1.jpeg)

There are a number of different ways we can gather data on our crayon box. 
* If you are on an iPhone Pro (iPhone 12, 14, 15, 16) you may use Apple's [Reality Composer]() iOS app or download their [Object Capture]() application. 
* If you do not have an iPhone, you may take at minimum 10 photos of your item. It is recommended to take between 50-100 photos for each object. Then, on [Reality Composer Pro]() we can compile the images and create a 3D model. 
* Alternatively, we can use other applications such as [PolyCam](), [Luma3D](), and [Abound]().

Other than our crayon box, we experimented with training individual crayons, the Ardunio UNO, stuffed animals, Raspberr Pi Pico, and whiteboard Expo markers. 

![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection2.png)

![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection3.png)

If you are working with a set of images, you may open up [Reality Composer Pro]() on the Mac and select **Create New Object Capture Model**. From there, a 3D model in a `.usdz` format will be created for you. You may also use existing 3D models, just make sure they are in `.usdz` format. 

![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection4.png)

Next, we can bring in our *.usdz* file into [CreateML](). It is important to make sure the objects are in the right dimensions in `cm`. If the dimensions are incorrect, you may modify them in a 3D modeling software such as [Blender]().


![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection5.png)


![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection6.png)

Finally, you can start training! It is expected to take some time. Once it is complete, you may export it as a **Reference Object** in `.referenceobject` format. Once you have your reference object, you may place the reference objects into your XCode project under the a folder named `Reference Objects`.

![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection7.png)

The reference objects selected for this project were a bit small. Creating a 3D model of small objects such as crayons and markers were difficult, especially with the object capture application. Taking individual photos of the crayons and then processing the photos in Reality Composer Pro also was difficult. 

![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection9.png)
![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection10.png)

After creating the reference objects, we are then able to add them to our **Reference Images** folder. From there, we will see our reference objects appear in our UI. 
![Apple Vision Pro](../.././components/public/images/objectdetection/objectdetection8.PNG)

### Hand Detection and Gestures
Squiggly uses Appleâ€™s hand-tracking APIs to detect when you pinch, tap, or perform other gestures in mid-air. This lets you draw, select tools, and interact with 3D elements without needing controllers.

### Eye Tracking
With visionOS eye-tracking, Squiggly can highlight objects or UI elements as you look at them, making interactions more natural. Your gaze can be combined with gestures to confirm selections, keeping your workflow smooth and hands-free.

### Object Detection Anchoring
In Squiggly, reference objects let the app recognize and track real-world itemsâ€”like crayonsâ€”so we can anchor 3D drawings in the correct position in space.

#### Method 1: Reality Composer Pro
Use Reality Composer Pro to import your `.arobject` or `.referenceobject` files. Once added, they can be placed directly into your scene with drag-and-drop simplicity. This is how Squiggly app implemented object anchoring. 

<Callout type="info" emoji="ðŸ’¡">
  <strong>Note:</strong> Want to learn more? Read more about using a reference object with Reality Composer Pro <strong><a href="https://developer.apple.com/documentation/visionos/using-a-reference-object-with-reality-composer-pro">here</a></strong>!
</Callout>

#### Method 2: Programmatically 
Load reference objects in Swift code using RealityKit. This allows you to dynamically choose which object to track at runtime, enabling advanced features like switching between multiple tracked objects.

<Callout type="info" emoji="ðŸ’¡">
  <strong>Note:</strong> Want to learn more? Read more about using a reference object with RealityKit <strong><a href="https://developer.apple.com/documentation/visionos/using-a-reference-object-with-realitykit">here</a></strong>!
</Callout>

#### Method 3: ARKit
For lower-level control, Squiggly can use ARKitâ€™s `ARReferenceObject` APIs. This is ideal if you need custom detection logic or want to integrate with AR session configuration directly.

<Callout type="info" emoji="ðŸ’¡">
  <strong>Note:</strong> Want to learn more? Read more about using a reference object with ARKit<strong><a href="https://developer.apple.com/documentation/visionos/using-a-reference-object-with-arkit">here</a></strong>!
</Callout>

### Local vs Cloud
All models used in the Squiggly app are using local models. For our learning purposes, we wanted to make sure the projects were able to run even without WiFi. 

you can run machine learning models either locally or in the cloud. When running locally, the Core ML model is downloaded to your device, allowing the app to work completely offline with low and consistent latency. All images stay on the headset, which means your data never leaves the device, making this option ideal for real-time interactions, privacy, and demos. However, the model size is limited by the app package and the computing power of the device.

```swift filename="ImageClasifier.swift" {6} copy
let request = VNClassifyImageRequest()
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

        do {
            try handler.perform([request])
            if let results = request.results as? [VNClassificationObservation] {
                for classification in results {
                    file.observations[classification.identifier] = classification.confidence
                }
            }
        } catch {
            print("Vision error: \(error)")
        }

console.log(a);
```

When running in the cloud, Squiggly sends snapshots to a Hugging Face endpoint where the model is hosted. This allows you to use larger or frequently updated models without having to ship a new app build. It does require an internet connection, and performance depends on network speed. Cloud inference is useful for rapid iteration, A/B testing, or when using architectures too large for on-device execution. By default, Squiggly runs locally, but if confidence in a classification is low or the local model isnâ€™t available, it can automatically fall back to the Hugging Face cloud option. You can also manually switch between local and cloud inference in the appâ€™s settings


```swift filename="CloudModelExample.swift" {3} copy
var request = URLRequest(url: URL(string: "https://api-inference.huggingface.co/models/your-namespace/your-model")!)
request.httpMethod = "POST"
request.setValue("Bearer \(tokenFromKeychain)", forHTTPHeaderField: "Authorization")
request.setValue("application/octet-stream", forHTTPHeaderField: "Content-Type")
request.httpBody = imageData // PNG/JPEG of the snapshot

let (data, _) = try await URLSession.shared.data(for: request)
struct HFLabel: Decodable { let label: String; let score: Double }
let preds = try JSONDecoder().decode([HFLabel].self, from: data)

```
