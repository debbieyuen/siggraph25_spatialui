## Hands-On-Lab

Let's get started with the hands-on component! 
Next to each Apple Vision Pro, you will find a crayon box. If you do not have a crayon box or would like one, you may find extras at the front of the classroom. Or, raise your hand and one of the instructors will support.

With the Mac, the Apple Vision Pro, and the crayon box, we will take you through an artistic journey while learning exploring the space of predictive UI or UI Understanding. 

### Setting Up

* **Step 1:** If you haven't alrady, put on the Apple Vision Pro and fit it. The Apple Vision Pros do not need passwords. For the best experience, please remain seated and adjust the straps to fit accordingly.

* **Step 2:** On the Macbooks, navigate to **XCode** and open the **Swiggly** Xcode project at `Documents/squiggly`. Make sure the project compiles correctly by selecting the **Run** button in the header.

* **Step 3:** On the Apple Vision Pro, find the **Squiggly App** and open it. You should see a welcome screen with a 3D model of SIGGRAPH's mascot, Pixel, animated. 

### Squiggly App 

Now that your Apple Vision Pro and the XCode project is set up, lets talk about the development and design of the app before diving into modfying the code and building to the headset! The Squiggly App is a demo app for learning purposes and is not a fully polished application. It is meant to be a starter project to support the visualization and imagination of machine learning and user interface design. 

Back to School season is here again. Take yourself all the way back to elementary school. It is art class and you and your classmates all have your own box of crayons. Try being creative and think of ways the user experience for physical-digital drawing can be. 

The goal of the Squiggly App is to not only explore the pipeline integration of machine learning when it comes to user interfaces in spatial contexts, but also to challenge the creative process of drawing in virtual reality. We begin with using our personal Photos Library for reference images to use while drawing. Then detecting crayon boxes and their individual crayons to draw in different colors then categorize the strokes patterns. 

The stroke pattern can then be exported and saved as package with 8 `.pngs` and 1 `.json` file. With 8 images taken from different views and a `.json` file, we can do more with our data such as creating a dataset, building a ML model, and creating a `.stl` file for 3D printing for example.

### Machine Learning
In the Squiggly app, machine learning was integrated in a number of different ways. In our `github.com/spatial-ml` repository and submodule and the `github.com/spatialui` repository you will find the following models we are experimenting with: 

| Model           | Description                                          |
|----------------------|------------------------------------------------------|
| [Vision Framework]()      | Used with PhotoPicker as default model        |
| [FastViT]()     | Secondary model that can be subsituted to be used with PhotoPicker |
| [MNIST Dataset with CoreML]()| Used to categorize drawing strokes              | 
| [Squigglers Model]() (Custom) | All reference objects for object detection | Apple Vision Pro                       |
| [Squiggly Model]() (Custom)  | Image classification which will later on be integrated as multimodal with `.json` files              |

### 3D UI and Rendering
Entering into the Squiggly app, you will immediately see a welcome screen. This screen in visionOS is called a Window. In the Squiggly App, we work with **SwiftUI** and **UIKit** for flat UI and for 3D text, RealityView components. 

Metal by Apple is really great for rendering. However, in this project, we did not use Metal for our project and solely relied on Reality Composer Pro and __. 